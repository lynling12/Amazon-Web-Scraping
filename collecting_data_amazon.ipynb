{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from lxml import html  \n",
    "import csv,os,json\n",
    "import requests\n",
    "from exceptions import ValueError\n",
    "from time import sleep\n",
    " \n",
    "def AmzonParser(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}\n",
    "    page = requests.get(url,headers=headers)\n",
    "    while True:\n",
    "        sleep(3)\n",
    "        try:\n",
    "            doc = html.fromstring(page.content)\n",
    "            XPATH_NAME = '//h1[@id=\"title\"]//text()'\n",
    "            XPATH_SALE_PRICE = '//span[contains(@id,\"ourprice\") or contains(@id,\"saleprice\")]/text()'\n",
    "            XPATH_ORIGINAL_PRICE = '//td[contains(text(),\"List Price\") or contains(text(),\"M.R.P\") or contains(text(),\"Price\")]/following-sibling::td/text()'\n",
    "            XPATH_CATEGORY = '//a[@class=\"a-link-normal a-color-tertiary\"]//text()'\n",
    "            XPATH_AVAILABILITY = '//div[@id=\"availability\"]//text()'\n",
    " \n",
    "            RAW_NAME = doc.xpath(XPATH_NAME)\n",
    "            RAW_SALE_PRICE = doc.xpath(XPATH_SALE_PRICE)\n",
    "            RAW_CATEGORY = doc.xpath(XPATH_CATEGORY)\n",
    "            RAW_ORIGINAL_PRICE = doc.xpath(XPATH_ORIGINAL_PRICE)\n",
    "            RAw_AVAILABILITY = doc.xpath(XPATH_AVAILABILITY)\n",
    " \n",
    "            NAME = ' '.join(''.join(RAW_NAME).split()) if RAW_NAME else None\n",
    "            SALE_PRICE = ' '.join(''.join(RAW_SALE_PRICE).split()).strip() if RAW_SALE_PRICE else None\n",
    "            CATEGORY = ' > '.join([i.strip() for i in RAW_CATEGORY]) if RAW_CATEGORY else None\n",
    "            ORIGINAL_PRICE = ''.join(RAW_ORIGINAL_PRICE).strip() if RAW_ORIGINAL_PRICE else None\n",
    "            AVAILABILITY = ''.join(RAw_AVAILABILITY).strip() if RAw_AVAILABILITY else None\n",
    " \n",
    "            if not ORIGINAL_PRICE:\n",
    "                ORIGINAL_PRICE = SALE_PRICE\n",
    " \n",
    "            if page.status_code!=200:\n",
    "                raise ValueError('captha')\n",
    "            data = {\n",
    "                    'NAME':NAME,\n",
    "                    'SALE_PRICE':SALE_PRICE,\n",
    "                    'CATEGORY':CATEGORY,\n",
    "                    'ORIGINAL_PRICE':ORIGINAL_PRICE,\n",
    "                    'AVAILABILITY':AVAILABILITY,\n",
    "                    'URL':url,\n",
    "                    }\n",
    " \n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print e\n",
    " \n",
    "def ReadAsin():\n",
    "    # AsinList = csv.DictReader(open(os.path.join(os.path.dirname(__file__),\"Asinfeed.csv\")))\n",
    "    AsinList = ['B0046UR4F4',\n",
    "    'B00JGTVU5A',\n",
    "    'B00GJYCIVK',\n",
    "    'B00EPGK7CQ',\n",
    "    'B00EPGKA4G',\n",
    "    'B00YW5DLB4',\n",
    "    'B00KGD0628',\n",
    "    'B00O9A48N2',\n",
    "    'B00O9A4MEW',\n",
    "    'B00UZKG8QU',]\n",
    "    extracted_data = []\n",
    "    for i in AsinList:\n",
    "        url = \"http://www.amazon.com/dp/\"+i\n",
    "        print \"Processing: \"+url\n",
    "        extracted_data.append(AmzonParser(url))\n",
    "        sleep(5)\n",
    "    f=open('data.json','w')\n",
    "    json.dump(extracted_data,f,indent=4)\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    ReadAsin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Written as part of https://www.scrapehero.com/how-to-scrape-amazon-product-reviews-using-python/\n",
    "from lxml import html  \n",
    "import json\n",
    "import requests\n",
    "import json,re\n",
    "from dateutil import parser as dateparser\n",
    "from time import sleep\n",
    "\n",
    "def ParseReviews(asin):\n",
    "    # Added Retrying \n",
    "    for i in range(5):\n",
    "        try:\n",
    "            #This script has only been tested with Amazon.com\n",
    "            amazon_url  = 'http://www.amazon.com/dp/'+asin\n",
    "            # Add some recent user agent to prevent amazon from blocking the request \n",
    "            # Find some chrome user agent strings  here https://udger.com/resources/ua-list/browser-detail?browser=Chrome\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}\n",
    "            page = requests.get(amazon_url,headers = headers)\n",
    "            page_response = page.text\n",
    "            parser = html.fromstring(page_response)\n",
    "            XPATH_AGGREGATE = '//span[@id=\"acrCustomerReviewText\"]'\n",
    "            XPATH_REVIEW_SECTION_1 = '//div[contains(@id,\"reviews-summary\")]'\n",
    "            XPATH_REVIEW_SECTION_2 = '//div[@data-hook=\"review\"]'\n",
    "            \n",
    "            XPATH_AGGREGATE_RATING = '//table[@id=\"histogramTable\"]//tr'\n",
    "            XPATH_PRODUCT_NAME = '//h1//span[@id=\"productTitle\"]//text()'\n",
    "            XPATH_PRODUCT_PRICE  = '//span[@id=\"priceblock_ourprice\"]/text()'\n",
    "            \n",
    "            raw_product_price = parser.xpath(XPATH_PRODUCT_PRICE)\n",
    "            product_price = ''.join(raw_product_price).replace(',','')\n",
    "            \n",
    "            raw_product_name = parser.xpath(XPATH_PRODUCT_NAME)\n",
    "            \n",
    "            product_name = ''.join(raw_product_name).strip()\n",
    "            total_ratings  = parser.xpath(XPATH_AGGREGATE_RATING)\n",
    "            reviews = parser.xpath(XPATH_REVIEW_SECTION_1)\n",
    "            if not reviews:\n",
    "                reviews = parser.xpath(XPATH_REVIEW_SECTION_2)\n",
    "            ratings_dict = {}\n",
    "            reviews_list = []\n",
    "            \n",
    "            if not reviews:\n",
    "                raise ValueError('unable to find reviews in page')\n",
    "            #grabing the rating  section in product page\n",
    "            for ratings in total_ratings:\n",
    "                extracted_rating = ratings.xpath('./td//a//text()')\n",
    "                if extracted_rating:\n",
    "                    rating_key = extracted_rating[0] \n",
    "                    raw_raing_value = extracted_rating[1]\n",
    "                    rating_value = raw_raing_value\n",
    "                    if rating_key:\n",
    "                        ratings_dict.update({rating_key:rating_value})\n",
    "            #Parsing individual reviews\n",
    "            for review in reviews:\n",
    "                XPATH_RATING  = './/i[@data-hook=\"review-star-rating\"]//text()'\n",
    "                XPATH_REVIEW_HEADER = './/a[@data-hook=\"review-title\"]//text()'\n",
    "                XPATH_REVIEW_POSTED_DATE = './/a[contains(@href,\"/profile/\")]/parent::span/following-sibling::span/text()'\n",
    "                XPATH_REVIEW_TEXT_1 = './/div[@data-hook=\"review-collapsed\"]//text()'\n",
    "                XPATH_REVIEW_TEXT_2 = './/div//span[@data-action=\"columnbalancing-showfullreview\"]/@data-columnbalancing-showfullreview'\n",
    "                XPATH_REVIEW_COMMENTS = './/span[@data-hook=\"review-comment\"]//text()'\n",
    "                XPATH_AUTHOR  = './/a[contains(@href,\"/profile/\")]/parent::span//text()'\n",
    "                XPATH_REVIEW_TEXT_3  = './/div[contains(@id,\"dpReviews\")]/div/text()'\n",
    "                raw_review_author = review.xpath(XPATH_AUTHOR)\n",
    "                raw_review_rating = review.xpath(XPATH_RATING)\n",
    "                raw_review_header = review.xpath(XPATH_REVIEW_HEADER)\n",
    "                raw_review_posted_date = review.xpath(XPATH_REVIEW_POSTED_DATE)\n",
    "                raw_review_text1 = review.xpath(XPATH_REVIEW_TEXT_1)\n",
    "                raw_review_text2 = review.xpath(XPATH_REVIEW_TEXT_2)\n",
    "                raw_review_text3 = review.xpath(XPATH_REVIEW_TEXT_3)\n",
    "                author = ' '.join(' '.join(raw_review_author).split()).strip('By')\n",
    "                #cleaning data\n",
    "                review_rating = ''.join(raw_review_rating).replace('out of 5 stars','')\n",
    "                review_header = ' '.join(' '.join(raw_review_header).split())\n",
    "                review_posted_date = dateparser.parse(''.join(raw_review_posted_date)).strftime('%d %b %Y')\n",
    "                review_text = ' '.join(' '.join(raw_review_text1).split())\n",
    "                \n",
    "                #grabbing hidden comments if present\n",
    "                if raw_review_text2:\n",
    "                    json_loaded_review_data = json.loads(raw_review_text2[0])\n",
    "                    json_loaded_review_data_text = json_loaded_review_data['rest']\n",
    "                    cleaned_json_loaded_review_data_text = re.sub('<.*?>','',json_loaded_review_data_text)\n",
    "                    full_review_text = review_text+cleaned_json_loaded_review_data_text\n",
    "                else:\n",
    "                    full_review_text = review_text\n",
    "                if not raw_review_text1:\n",
    "                    full_review_text = ' '.join(' '.join(raw_review_text3).split())\n",
    "                \n",
    "                raw_review_comments = review.xpath(XPATH_REVIEW_COMMENTS)\n",
    "                review_comments = ''.join(raw_review_comments)\n",
    "                review_comments = re.sub('[A-Za-z]','',review_comments).strip()\n",
    "                review_dict = {\n",
    "                                 'review_comment_count':review_comments,\n",
    "                                  'review_text':full_review_text,\n",
    "                                 'review_posted_date':review_posted_date,\n",
    "                                 'review_header':review_header,\n",
    "                                 'review_rating':review_rating,\n",
    "                                 'review_author':author,}\n",
    "                reviews_list.append(review_dict)\n",
    "            data = {\n",
    "                      'ratings':ratings_dict,\n",
    "                     'reviews':reviews_list,\n",
    "                      'url':amazon_url,\n",
    "                     'price':product_price,\n",
    "                      'name':product_name,}\n",
    "            return data\n",
    "        except ValueError:\n",
    "            print \"Retrying to get the correct response\"\n",
    "            \n",
    "    return {\"error\":\"failed to process the page\",\"asin\":asin}\n",
    "\n",
    "def ReadAsin():\n",
    "    #Add your own ASINs here \n",
    "    AsinList = ['B01ETPUQ6E','B017HW9DEW']\n",
    "    extracted_data = []\n",
    "    for asin in AsinList:\n",
    "        print \"Downloading and processing page http://www.amazon.com/dp/\"+asin\n",
    "        extracted_data.append(ParseReviews(asin))\n",
    "        sleep(5)\n",
    "    f=open('data1.json','w')\n",
    "    json.dump(extracted_data,f,indent=4)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ReadAsin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: http://www.amazon.com/dp/B015H5MDIO\n",
      "Processing: http://www.amazon.com/dp/B01H0S58M4\n",
      "Processing: http://www.amazon.com/dp/B077MRBVRB\n",
      "Processing: http://www.amazon.com/dp/B01N3CM6ZH\n",
      "Processing: http://www.amazon.com/dp/B074XMZ7MN\n",
      "Processing: http://www.amazon.com/dp/B01DJYRCK4\n",
      "Processing: http://www.amazon.com/dp/B01HPE05BM\n",
      "Processing: http://www.amazon.com/dp/B01N0RHZAW\n",
      "Processing: http://www.amazon.com/dp/B0728H2YP5\n",
      "Processing: http://www.amazon.com/dp/B06XZS76C7\n",
      "Processing: http://www.amazon.com/dp/B01HFNML10\n",
      "Processing: http://www.amazon.com/dp/B075JYG2TB\n",
      "Processing: http://www.amazon.com/dp/B076JGCVSP\n",
      "Processing: http://www.amazon.com/dp/B06XGY6KCL\n",
      "Processing: http://www.amazon.com/dp/B0746YGV35\n",
      "Processing: http://www.amazon.com/dp/B00Y3P0K1M\n",
      "Processing: http://www.amazon.com/dp/B01KU4N4L2\n",
      "Processing: http://www.amazon.com/dp/B06XKLMMNM\n",
      "Processing: http://www.amazon.com/dp/B00JI6NCCK\n",
      "Processing: http://www.amazon.com/dp/B06Y13N1PY\n",
      "Processing: http://www.amazon.com/dp/B076DDRSBK\n",
      "Processing: http://www.amazon.com/dp/B019GTZ5ZA\n",
      "Processing: http://www.amazon.com/dp/B01MDKROXW\n",
      "Processing: http://www.amazon.com/dp/B00JI6NCCK\n",
      "Processing: http://www.amazon.com/dp/B01MDKROXW\n",
      "Processing: http://www.amazon.com/dp/B075JYG2TB\n",
      "Processing: http://www.amazon.com/dp/B01BMULFAA\n",
      "Processing: http://www.amazon.com/dp/B00UVHTGJ8\n",
      "Processing: http://www.amazon.com/dp/B01MSZQLML\n",
      "Processing: http://www.amazon.com/dp/B06Y5HCDVL\n",
      "Processing: http://www.amazon.com/dp/B01NCKIU70\n",
      "Processing: http://www.amazon.com/dp/B01HPE05BM\n",
      "Processing: http://www.amazon.com/dp/B06XZS76C7\n",
      "Processing: http://www.amazon.com/dp/B073XWTDN3\n",
      "Processing: http://www.amazon.com/dp/B072Q5M6H5\n",
      "Processing: http://www.amazon.com/dp/B06XGY6KCL\n",
      "Processing: http://www.amazon.com/dp/B075JYG2TB\n",
      "Processing: http://www.amazon.com/dp/B076HJ8C1H\n",
      "Processing: http://www.amazon.com/dp/B073HQJF29\n",
      "Processing: http://www.amazon.com/dp/B072XHN1C3\n",
      "Processing: http://www.amazon.com/dp/B073HQJF29\n",
      "Processing: http://www.amazon.com/dp/B06Y13N1PY\n",
      "Processing: http://www.amazon.com/dp/B0747733JH\n",
      "Processing: http://www.amazon.com/dp/B078H9WZ5S\n",
      "Processing: http://www.amazon.com/dp/B01KVY2FSE\n",
      "Processing: http://www.amazon.com/dp/B0728H2YP5\n",
      "Processing: http://www.amazon.com/dp/B06XWQ3YNJ\n",
      "Processing: http://www.amazon.com/dp/B0784HWPN6\n",
      "Processing: http://www.amazon.com/dp/B01KVY2FSE\n",
      "Processing: http://www.amazon.com/dp/B0716YM2RL\n",
      "Processing: http://www.amazon.com/dp/B06XKLMMNM\n",
      "Processing: http://www.amazon.com/dp/B01N98WU1E\n",
      "Processing: http://www.amazon.com/dp/B01NAVHONO\n",
      "Processing: http://www.amazon.com/dp/B072LTTBVY\n",
      "Processing: http://www.amazon.com/dp/B01HPE05B2\n",
      "Processing: http://www.amazon.com/dp/B076JGCVSP\n",
      "Processing: http://www.amazon.com/dp/B01IB7QFBG\n",
      "Processing: http://www.amazon.com/dp/B01N2QRGWG\n",
      "Processing: http://www.amazon.com/dp/B01H0S58M4\n",
      "Processing: http://www.amazon.com/dp/B077H3W47V\n",
      "Processing: http://www.amazon.com/dp/B00UZKG8QU\n",
      "Processing: http://www.amazon.com/dp/B075SZ2DY5\n"
     ]
    }
   ],
   "source": [
    "from lxml import html  \n",
    "import csv,os,json\n",
    "import requests\n",
    "from exceptions import ValueError\n",
    "from time import sleep\n",
    " \n",
    "def AmzonParser(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}\n",
    "    page = requests.get(url,headers=headers)\n",
    "    while True:\n",
    "        sleep(3)\n",
    "        try:\n",
    "            doc = html.fromstring(page.content)\n",
    "            XPATH_Brand_NAME = '//a[@id=\"bylineInfo\"]//text()'\n",
    "            XPATH_NAME = '//h1[@id=\"title\"]//text()'\n",
    "            XPATH_SALE_PRICE = '//span[contains(@id,\"ourprice\") or contains(@id,\"saleprice\")]/text()'\n",
    "            XPATH_ORIGINAL_PRICE = '//td[contains(text(),\"List Price\") or contains(text(),\"M.R.P\") or contains(text(),\"Price\")]/following-sibling::td/text()'\n",
    "            XPATH_CATEGORY = '//a[@class=\"a-link-normal a-color-tertiary\"]//text()'\n",
    "            XPATH_AVAILABILITY = '//div[@id=\"availability\"]//text()'\n",
    " \n",
    "            RAW__BRAND_NAME = doc.xpath(XPATH_Brand_NAME)\n",
    "            RAW_NAME = doc.xpath(XPATH_NAME)\n",
    "            RAW_SALE_PRICE = doc.xpath(XPATH_SALE_PRICE)\n",
    "            RAW_CATEGORY = doc.xpath(XPATH_CATEGORY)\n",
    "            RAW_ORIGINAL_PRICE = doc.xpath(XPATH_ORIGINAL_PRICE)\n",
    "            RAw_AVAILABILITY = doc.xpath(XPATH_AVAILABILITY)\n",
    " \n",
    "            BRAND_NAME = ' '.join(''.join( RAW__BRAND_NAME).split()) if  RAW__BRAND_NAME else None\n",
    "            NAME = ' '.join(''.join(RAW_NAME).split()) if RAW_NAME else None\n",
    "            SALE_PRICE = ' '.join(''.join(RAW_SALE_PRICE).split()).strip() if RAW_SALE_PRICE else None\n",
    "            CATEGORY = ' > '.join([i.strip() for i in RAW_CATEGORY]) if RAW_CATEGORY else None\n",
    "            ORIGINAL_PRICE = ''.join(RAW_ORIGINAL_PRICE).strip() if RAW_ORIGINAL_PRICE else None\n",
    "            AVAILABILITY = ''.join(RAw_AVAILABILITY).strip() if RAw_AVAILABILITY else None\n",
    " \n",
    "            if not ORIGINAL_PRICE:\n",
    "                ORIGINAL_PRICE = SALE_PRICE\n",
    " \n",
    "            if page.status_code!=200:\n",
    "                raise ValueError('captha')\n",
    "            data = {'BRAND_NAME':BRAND_NAME,\n",
    "                    'NAME':NAME,\n",
    "                    'SALE_PRICE':SALE_PRICE,\n",
    "                    'CATEGORY':CATEGORY,\n",
    "                    'ORIGINAL_PRICE':ORIGINAL_PRICE,\n",
    "                    'AVAILABILITY':AVAILABILITY,\n",
    "                    'URL':url,\n",
    "                    }\n",
    " \n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print e\n",
    " \n",
    "def ReadAsin():\n",
    "    # AsinList = csv.DictReader(open(os.path.join(os.path.dirname(__file__),\"Asinfeed.csv\")))\n",
    "    AsinList = ['B015H5MDIO',\n",
    "    'B01H0S58M4', 'B077MRBVRB', 'B01N3CM6ZH', 'B074XMZ7MN', 'B01DJYRCK4', 'B01HPE05BM','B01N0RHZAW',\n",
    "    'B0728H2YP5', 'B06XZS76C7', 'B01HFNML10','B075JYG2TB', 'B076JGCVSP', 'B06XGY6KCL', 'B0746YGV35',\n",
    "    'B00Y3P0K1M', 'B01KU4N4L2', 'B06XKLMMNM', 'B00JI6NCCK', 'B06Y13N1PY', 'B076DDRSBK','B019GTZ5ZA',\n",
    "    'B01MDKROXW', 'B00JI6NCCK', 'B01MDKROXW', 'B075JYG2TB', 'B01BMULFAA', 'B00UVHTGJ8', 'B01MSZQLML',\n",
    "    'B06Y5HCDVL','B01NCKIU70', 'B01HPE05BM', 'B06XZS76C7', 'B073XWTDN3', 'B072Q5M6H5', 'B06XGY6KCL',\n",
    "    'B075JYG2TB','B076HJ8C1H', 'B073HQJF29', 'B072XHN1C3', 'B073HQJF29', 'B06Y13N1PY','B0747733JH', 'B078H9WZ5S',\n",
    "    'B01KVY2FSE','B0728H2YP5', 'B06XWQ3YNJ', 'B0784HWPN6', 'B01KVY2FSE', 'B0716YM2RL', 'B06XKLMMNM', 'B01N98WU1E',\n",
    "    'B01NAVHONO','B072LTTBVY', 'B01HPE05B2', 'B076JGCVSP', 'B01IB7QFBG', 'B01N2QRGWG', 'B01H0S58M4','B077H3W47V',\n",
    "    'B00UZKG8QU','B075SZ2DY5',]\n",
    "    extracted_data = []\n",
    "    for i in AsinList:\n",
    "        url = \"http://www.amazon.com/dp/\"+i\n",
    "        print \"Processing: \"+url\n",
    "        extracted_data.append(AmzonParser(url))\n",
    "        sleep(5)\n",
    "    f=open('data.json','w')\n",
    "    json.dump(extracted_data,f,indent=4)\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    ReadAsin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
